{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "file_locations = {\n",
    "    'tpch': ['customer/orders_customer_optimal_rep.csv', 'orders/customer_orders_optimal_rep.csv', \n",
    "             'orders/lineitem_orders_optimal_rep.csv', 'part/lineitem_part_optimal_rep.csv', \n",
    "             'part/partsupp_part_optimal_rep.csv', 'partsupp/lineitem_partsupp_optimal_rep.csv', \n",
    "             'partsupp/part_partsupp_optimal_rep.csv', 'partsupp/supplier_partsupp_optimal_rep.csv', \n",
    "             'supplier/lineitem_supplier_optimal_rep.csv', 'supplier/partsupp_supplier_optimal_rep.csv'],\n",
    "    'imdb': ['cast_info/title_cast_info_optimal_rep.csv',\n",
    "             'movie_companies/title_movie_companies_optimal_rep.csv', 'movie_info/title_movie_info_optimal_rep.csv', \n",
    "             'movie_info_idx/title_movie_info_idx_optimal_rep.csv', 'movie_keyword/title_movie_keyword_optimal_rep.csv',\n",
    "             'title/cast_info_title_optimal_rep.csv', 'title/movie_companies_title_optimal_rep.csv',\n",
    "             'title/movie_info_idx_title_optimal_rep.csv', 'title/movie_info_title_optimal_rep.csv',\n",
    "             'title/movie_keyword_title_optimal_rep.csv'\n",
    "            ],\n",
    "    'ssb': ['customer/lineorder_customer_optimal_rep.csv', 'ddate/lineorder_ddate_optimal_rep.csv', \n",
    "           'part/lineorder_part_optimal_rep.csv', 'supplier/lineorder_supplier_optimal_rep.csv'\n",
    "           ]\n",
    "}\n",
    "\n",
    "\n",
    "datasets = ['ssb', 'tpch', 'imdb']\n",
    "\n",
    "\n",
    "\n",
    "dfs = []\n",
    "\n",
    "\n",
    "for d in datasets:\n",
    "    for f in file_locations[d]:\n",
    "        dfs.append(pd.read_csv(os.path.join(\"../data/\", d, f)))\n",
    "\n",
    "ds = pd.concat(dfs)\n",
    "ds\n",
    "\n",
    "ds['left+right'] = ds['left_cardinality'] + ds['base_cardinality']\n",
    "ds['left*right'] = ds['left_cardinality'] * ds['base_cardinality']\n",
    "ds['left/right'] = ds['left_cardinality'] / ds['base_cardinality']\n",
    "ds['left-right'] = ds['left_cardinality'] - ds['base_cardinality']\n",
    "ds['left^2'] = ds['left_cardinality'] * ds['left_cardinality']\n",
    "ds['right^2'] = ds['base_cardinality'] * ds['base_cardinality']\n",
    "ds['left*logleft'] = ds['left_cardinality'] * np.log(ds['left_cardinality'])\n",
    "ds['right*logright'] = ds['base_cardinality'] * np.log(ds['base_cardinality'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygam import LinearGAM, s, f\n",
    "from pygam.datasets import wage\n",
    "from interpret.glassbox import ExplainableBoostingClassifier, ExplainableBoostingRegressor\n",
    "\n",
    "m_regression_model = ExplainableBoostingRegressor\n",
    "m_classification_model = ExplainableBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:27: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:30: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n"
     ]
    }
   ],
   "source": [
    "all_features = ['left_cardinality', 'base_cardinality',\n",
    "       'selectivity_on_indexed_attr', 'left_ordered', 'base_ordered',\n",
    "       'result_size', 'sel_on_indexed_attr_with_join_predicate']\n",
    "\n",
    "key_features = ['left_cardinality', 'base_cardinality',\n",
    "       'result_size']\n",
    "\n",
    "augmented_features = all_features + \\\n",
    "    ['left+right', 'left*right', 'left/right',\n",
    "        'left-right', 'left-right', 'left^2', 'right^2', 'left*logleft', 'right*logright']\n",
    "\n",
    "features = augmented_features\n",
    "\n",
    "regression_targets = ['hj_idx_cost', 'hj_seq_cost', 'nl_idx_cost', 'nl_seq_cost', 'mj_idx_cost', 'mj_seq_cost']\n",
    "classification_target = ['optimal_decision']\n",
    "\n",
    "\n",
    "X = ds[features]\n",
    "y = ds['optimal_decision']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,random_state=1)\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "# scaler = preprocessing.StandardScaler().fit(X_test)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/interpret/glassbox/ebm/ebm.py:801: UserWarning: Multiclass is still experimental. Subject to change per release.\n",
      "  warn(\"Multiclass is still experimental. Subject to change per release.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.12%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = m_classification_model().fit(X_train, y_train)\n",
    "\n",
    "acc = np.sum(clf.predict(X_test) == y_test) / len(y_test)\n",
    "print(f\"Test accuracy: %.2f%%\" % (acc * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shap\n",
    "\n",
    "# x_ds_train = pd.DataFrame(data=X_train[0:500], columns=features)\n",
    "\n",
    "# shap_values = shap.TreeExplainer(clf).shap_values(x_ds_train)\n",
    "# shap.summary_plot(shap_values, x_ds_train, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect all the regressors\n",
    "regressors = {}\n",
    "\n",
    "features = augmented_features\n",
    "\n",
    "regression_targets = ['hj_idx_cost', 'hj_seq_cost', 'nl_idx_cost', 'nl_seq_cost','mj_idx_cost', 'mj_seq_cost']\n",
    "classification_target = ['optimal_decision']\n",
    "\n",
    "\n",
    "X = ds[features].to_numpy()\n",
    "y = ds[regression_targets + classification_target]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# quantile_transformer = preprocessing.QuantileTransformer(random_state=0).fit(X_train)\n",
    "# X_train = quantile_transformer.transform(X_train)\n",
    "# X_test = quantile_transformer.transform(X_test)\n",
    "\n",
    "\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash join + index scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in percentage: +- 18.71261786236404%\n"
     ]
    }
   ],
   "source": [
    "c_y_train = np.log(y_train['hj_idx_cost'].to_numpy().reshape(-1, 1))\n",
    "c_y_test = y_test['hj_idx_cost'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "# y_scaler = preprocessing.MinMaxScaler()\n",
    "# c_y_train = y_scaler.fit_transform(c_y_train)\n",
    "\n",
    "rgr = m_regression_model().fit(X_train, c_y_train)\n",
    "regressors['hj_idx_cost'] = rgr\n",
    "loss_in_percentage = np.average(np.abs(np.exp(rgr.predict(X_test).reshape(-1,1)) - c_y_test) / c_y_test)\n",
    "print(f\"Loss in percentage: +- {loss_in_percentage * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, importance in zip(features, rgr.feature_importances_):\n",
    "#     print(name, \"=\", importance)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# importances = rgr.feature_importances_\n",
    "# indices = np.argsort(importances)\n",
    "\n",
    "# plt.title('Feature importances for hash join + index scan')\n",
    "# plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "# plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "# plt.xlabel('Relative Importance')\n",
    "# plt.show()\n",
    "\n",
    "# c_y_train = y_train['hj_idx_cost'].to_numpy().reshape(-1, 1)\n",
    "# c_y_test = y_test['hj_idx_cost'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "# y_scaler = preprocessing.QuantileTransformer()\n",
    "# c_y_train = y_scaler.fit_transform(c_y_train)\n",
    "\n",
    "# rgr = m_regression_model().fit(X_train, c_y_train)\n",
    "# regressors['hj_idx_cost'] = rgr\n",
    "# loss_in_percentage = np.average(np.abs(y_scaler.inverse_transform(rgr.predict(X_test).reshape(-1,1)) - c_y_test) / c_y_test)\n",
    "# print(f\"Loss in percentage: +- {loss_in_percentage * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash join + seq scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in percentage: +- 11.229488212949125%\n"
     ]
    }
   ],
   "source": [
    "c_y_train = np.log(y_train['hj_seq_cost'].to_numpy().reshape(-1, 1))\n",
    "c_y_test = y_test['hj_seq_cost'].to_numpy().reshape(-1, 1)\n",
    "rgr = m_regression_model().fit(X_train, c_y_train)\n",
    "regressors['hj_seq_cost'] = rgr\n",
    "loss_in_percentage = np.average(np.abs(np.exp(rgr.predict(X_test).reshape(-1,1)) - c_y_test) / c_y_test)\n",
    "print(f\"Loss in percentage: +- {loss_in_percentage * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[11.09884062],\n",
       "       [11.73346126],\n",
       "       [11.6535318 ],\n",
       "       ...,\n",
       "       [12.11420642],\n",
       "       [11.7524864 ],\n",
       "       [ 9.95634816]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for name, importance in zip(features, rgr.feature_importances_):\n",
    "#     print(name, \"=\", importance)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# importances = rgr.feature_importances_\n",
    "# indices = np.argsort(importances)\n",
    "\n",
    "# plt.title('Feature importances for hash join + index scan')\n",
    "# plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "# plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "# plt.xlabel('Relative Importance')\n",
    "# plt.show()\n",
    "rgr.predict(X_test).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested loop + idx scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in percentage: +- 22.567895832729576%\n"
     ]
    }
   ],
   "source": [
    "c_y_train, c_y_test = np.log(y_train['nl_idx_cost'].to_numpy().reshape(-1, 1)), y_test['nl_idx_cost'].to_numpy().reshape(-1, 1)\n",
    "rgr = m_regression_model().fit(X_train, c_y_train)\n",
    "regressors['nl_idx_cost'] = rgr\n",
    "loss_in_percentage = np.average( np.abs( np.exp(rgr.predict(X_test).reshape(-1,1)) - c_y_test) / c_y_test)\n",
    "print(f\"Loss in percentage: +- {loss_in_percentage * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### nested loop + seq scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in percentage: +- 7.967411028915744%\n"
     ]
    }
   ],
   "source": [
    "c_y_train = np.log(y_train['nl_seq_cost'].to_numpy().reshape(-1, 1))\n",
    "c_y_test = y_test['nl_seq_cost'].to_numpy().reshape(-1, 1)\n",
    "rgr = m_regression_model().fit(X_train, c_y_train)\n",
    "regressors['nl_seq_cost'] = rgr\n",
    "loss_in_percentage = np.average(np.abs(np.exp(rgr.predict(X_test).reshape(-1,1)) - c_y_test) / c_y_test)\n",
    "print(f\"Loss in percentage: +- {loss_in_percentage * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, importance in zip(features, rgr.feature_importances_):\n",
    "#     print(name, \"=\", importance)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# importances = rgr.feature_importances_\n",
    "# indices = np.argsort(importances)\n",
    "\n",
    "# plt.title('Feature importances for hash join + index scan')\n",
    "# plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "# plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "# plt.xlabel('Relative Importance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge join + index scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in percentage: +- 16.843268118325952%\n"
     ]
    }
   ],
   "source": [
    "c_y_train = np.log(y_train['mj_idx_cost'].to_numpy().reshape(-1, 1))\n",
    "c_y_test = y_test['mj_idx_cost'].to_numpy().reshape(-1, 1)\n",
    "rgr = m_regression_model().fit(X_train, c_y_train)\n",
    "regressors['mj_idx_cost'] = rgr\n",
    "\n",
    "loss_in_percentage = np.average(np.abs(np.exp(rgr.predict(X_test).reshape(-1,1)) - c_y_test) / c_y_test)\n",
    "print(f\"Loss in percentage: +- {loss_in_percentage * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, importance in zip(features, rgr.feature_importances_):\n",
    "#     print(name, \"=\", importance)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# importances = rgr.feature_importances_\n",
    "# indices = np.argsort(importances)\n",
    "\n",
    "# plt.title('Feature importances for hash join + index scan')\n",
    "# plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "# plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "# plt.xlabel('Relative Importance')\n",
    "# plt.show()\n",
    "# print(rgr.predict(X_test)[-4])\n",
    "# print(c_y_test.iloc[-4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge join + seq scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss in percentage: +- 10.594037839601958%\n"
     ]
    }
   ],
   "source": [
    "c_y_train = np.log(y_train['mj_seq_cost'].to_numpy().reshape(-1, 1))\n",
    "c_y_test = y_test['mj_seq_cost'].to_numpy().reshape(-1, 1)\n",
    "\n",
    "rgr = m_regression_model().fit(X_train, c_y_train)\n",
    "regressors['mj_seq_cost'] = rgr\n",
    "\n",
    "loss_in_percentage = np.average(np.abs(np.exp(rgr.predict(X_test).reshape(-1,1)) - c_y_test) / c_y_test)\n",
    "print(f\"Loss in percentage: +- {loss_in_percentage * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for name, importance in zip(features, rgr.feature_importances_):\n",
    "#     print(name, \"=\", importance)\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# importances = rgr.feature_importances_\n",
    "# indices = np.argsort(importances)\n",
    "\n",
    "# plt.title('Feature importances for hash join + index scan')\n",
    "# plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "# plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "# plt.xlabel('Relative Importance')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Regression model to do classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy using all learned costs: 82.55%\n"
     ]
    }
   ],
   "source": [
    "predict_test = []\n",
    "operators = ['hj_idx_cost', 'hj_seq_cost', 'nl_idx_cost', 'nl_seq_cost','mj_idx_cost', 'mj_seq_cost']\n",
    "\n",
    "for op_idx, op in enumerate(operators) :\n",
    "    predict_test.append(regressors[op].predict(X_test))\n",
    "    \n",
    "results = np.stack(predict_test, axis=1)\n",
    "acc = np.sum(np.argmin(results, axis=1).reshape(-1,1).flatten() == y_test['optimal_decision'].to_numpy().flatten()) / len(y_test)\n",
    "print(\"Test accuracy using all learned costs: %.2f%%\" % (acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
